{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence of a Bayesian Neural Network\n",
    "This notebook examines the confidence calculation of a *Bayesian* neural network. I will use the *MNIST* and *FashionMNIST* datasets to demonstrate how confidence within the prediction process can help to avoid misclassification. `PyTorch` and `Pyro` are used for the probabilistic programming part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Bayesian Inference\n",
    "Before we start to implement the *Bayesian* neural network lets have a look at the theory behind *Approximate Bayesian Inference*. \n",
    "#### Bayesian inference\n",
    "The *prior* $p(\\theta)$ is our prior believe of the (probability) distribution of the parameters $\\theta$, which we have to identify first.\n",
    "\n",
    "The *likelihood* $p(y_{1:N} | \\theta)$ tells us, how our data $y$ interacts with / relates to our parameter $\\theta$ for $N$ data points.\n",
    "\n",
    "**Model:** The *likelihood* and the *prior* makes the model! This tells us how we can simulate from our data.\n",
    "\n",
    "The *prior* as a marginal and the *likelihood* as a conditional together makes a *joined distribution* $p(y_{1:N},\\theta)$.\n",
    "\n",
    "$p(y | \\theta) \\: p(\\theta) = p(y,\\theta)$\n",
    "\n",
    "This joined distribution can be also written as $p(\\theta, y) = p(\\theta | y) p(y)$\n",
    "\n",
    "Which gives us $p(\\theta | y) = \\frac{p(\\theta, y)}{p(y)}$. This is equivalent to \n",
    "\n",
    "$$p(\\theta | y_{1:N}) = \\frac{p(y_{1:N} | \\theta) p(\\theta)}{p(y_{1:N})}$$ \n",
    "\n",
    "which is called the ***Bayes Theorem***.\n",
    "\n",
    "$p(y_{1:N})$ is the normalizing constant and is called the *evidence*. It is the probability of the data. It is the marginal of the joined probability $p(\\theta , y)$ with $\\theta$ integrated out. Therefore it can be written as: $\\int p(y_{1:N} | \\theta) \\: p(\\theta) \\: d\\theta$\n",
    "\n",
    "$p(\\theta | y_{1:N})$ is called the *posterior* and is exactly what we want to infer by Bayesian Inference.\n",
    "\n",
    "Finally the Bayes Theorem can be written as: $$p(\\theta | y_{1:N}) = \\frac{p(y_{1:N} | \\theta) p(\\theta)}{\\int p(y_{1:N} | \\theta) p(\\theta)d\\theta}$$\n",
    "\n",
    "Due to the possibly high dimension of $\\theta$ and therefore the high dimensionality of the *posterior* we want to reduce our result to a point estimate and a notion of its distribution / uncertainty. This is mostly the *mean* and its *(co)variance*.\n",
    "\n",
    "**Goal of Bayesian inference:** The goal of Bayesian inference is calculating the *posterior* and presenting a *mean* and a *(co)variance* of the calculated distribution $p(\\theta | y_{1:N})$.\n",
    "\n",
    "#### Problems of the Bayes Theorem\n",
    "1. High dimensional integration of $\\theta$ over the *evidence* to get the *posterior*. \n",
    "2. High dimensional integration of $\\theta$ over the *posterior* to get a posterior *mean* and *(co)variance*.\n",
    "\n",
    "Because of the intractability of the high dimensional integrations mentioned above the question is how do we get a *posterior* and its *mean* and *(co)variance* if we cannot calculate it directly?\n",
    "\n",
    "*Approximate Bayesian Inference* it the solution to this problem.\n",
    "\n",
    "#### Approximate the *posterior* by using Variational Bayes\n",
    "If we cannot calculate the *posterior* we are going to approximate it by using *Variational Bayes*, which is an optimization approach. \n",
    "\n",
    "We denote the \"optimal\" approximated *posterior* by $q^*$. An \"optimal\" approximated *posterior* is the closest $q$ to the exact *posterior* $p(\\theta|y)$ for which we can calculate a *mean* and *(co)variance*. (Remember our goal is to calculate / approximate a *mean* and *(co)variance* as our result.) \n",
    "<br><br>\n",
    "<!-- <figure>\n",
    "    <img src='pictures/variational-bayes.png' width=400 />\n",
    "    <figcaption>Caption goes here</figcaption>\n",
    "</figure>\n",
    " -->\n",
    "<figure>\n",
    "    <img src='../pictures/variational-bayes.png' width=400 align=left />\n",
    "    <div style='width: 400px; text-align: center;'>Tamara Broderick - Tutorial \"Variational Bayes and beyond: Bayesian inference for big data\" - ICML 2018.</div>\n",
    "</figure>\n",
    "\n",
    "In the case of *Variational Bayes* $q^*$ is: $$q^* = argmin_{q\\in Q} KL(q(\\theta) \\parallel p(\\theta | y))$$\n",
    "\n",
    "where KL is the *Kullback-Leibler (KL)* divergence and $Q$ is the bulk of appropriate or \"nice\" distributions for which we can calculate the *mean* and the *variance*.\n",
    "\n",
    "#### Advantage of using KL divergence\n",
    "Lets look at the definition of the KL divergence:\n",
    "\n",
    "$$ KL(q(\\theta) \\parallel p(\\theta | y) := \\int q(\\theta) \\: log \\: \\frac{q(\\theta)}{p(\\theta|y)} \\: d\\theta$$\n",
    "\n",
    "where $q(\\theta)$ is the approximating distribution and $p(\\theta|y)$ is the exact *posterior* we want to approximate. \n",
    "\n",
    "Now the problem with the KL divergence is that we do not know the exact *posterior* so how can we approximate something that we don't know. To answer this question we need to do some algebraic manipulation and we start with replacing $p(\\theta|y)$ in the denominator by $\\frac {p(\\theta, y)}{p(y)}$:\n",
    "\n",
    "<!-- $$ KL(q(\\theta) \\parallel p(\\theta | y) := \\int q(\\theta)\\: log \\: \\frac{q(\\theta)p(y)}{p(\\theta,y)}\\: d\\theta$$ \n",
    " -->\n",
    "\n",
    "$$ KL(q(\\theta) \\parallel p(\\theta | y) := \\int q(\\theta) \\: log \\frac{p(y)}{\\frac{p(\\theta,y)}{q(\\theta)}} \\: d\\theta$$ \n",
    "\n",
    "now using the fact that $log\\frac{a}{b} = log(a) - log(b)$ (quotient rule) we get:\n",
    "\n",
    "$$ = \\int q(\\theta)\\: (log \\: p(y) - log \\: \\frac{p(\\theta,y)}{q(\\theta)})\\:d\\theta$$\n",
    "\n",
    "$$ = \\int q(\\theta)\\: log \\: p(y)\\: d\\theta - \\int q(\\theta) log \\: \\frac{p(\\theta,y)}{q(\\theta)}\\:d\\theta$$\n",
    "\n",
    "where $ \\int q(\\theta) \\: log \\, p(y) \\: d(\\theta)$ integrates to $log \\, p(y)$ because $q$ is a distribution and therefore $\\int q(\\theta) \\: d\\theta$ integrates to $1$. So we get:\n",
    "\n",
    "$$ = log\\: p(y) - \\int q(\\theta)\\: log \\: \\frac{p(\\theta,y)}{q(\\theta)}\\:d\\theta$$\n",
    "\n",
    "The first term is the log of the evidence and does not contain $q$. As we are minimizing the KL divergence over $q$ this first term $log\\: p(y)$ does not have any impact.\n",
    "\n",
    "The second term is called *Evidence Lower Bound (ELBO)* and it contains only elements that we know. The joined distribution $p(\\theta,y)$ is the *likelihood* times the *prior* and this is our input. This term can be optimized.\n",
    "\n",
    "Due to the fact that the $KL \\ge 0$ we have $log \\, p(y) \\ge ELBO$ and it is a lower bound for the $log$ of the *evidence*.\n",
    "\n",
    "So now we have: \n",
    "\n",
    "$$q^* = argmin_{q\\in Q} KL(q(\\theta) \\parallel p(\\theta | y))$$ with \n",
    "\n",
    "$$ KL(q(\\theta) \\parallel p(\\theta | y) := log\\: p(y) - \\int q(\\theta)\\: log \\: \\frac{p(\\theta,y)}{q(\\theta)}\\:d\\theta $$ \n",
    "\n",
    "and we get: \n",
    "\n",
    "$$q^* = argmax_{q \\in Q} \\,ELBO(q)$$\n",
    "\n",
    "**Therefore minimizing the $KL$ divergence is equivalent to maximizing the $ELBO$.**\n",
    "\n",
    "#### Conclusion\n",
    "We started with the problem of minimizing the KL divergence between $q(\\theta)$ and our exact *posterior* $p(\\theta|y)$ which we don't know and ended up with maximizing the $ELBO$. This is the exact same problem with the benefit that we can solve it by optimization.\n",
    "\n",
    "\n",
    "#### Mean-field Variational Bayes \n",
    "\n",
    "Last but not least we have to check what are appropriate or \"nice\", as called in the depiction above, distributions. As mentioned at the beginning of this explanation, we are looking for a *mean* and a *variance* and therefore we need distributions for which we can easily compute *means* and *variances*. This requirement is perfectly fulfilled by low dimensional, exponential distributions.\n",
    "\n",
    "Starting with a high dimensional parameter vector $\\theta$, we break it down into $J$ smaller components. Now the approximating distribution $q(\\theta)$ factorizes over these $J$ components to calculate the approximation. This is the *Mean-field variational Bayes (MFVB)* assumption. \n",
    "\n",
    "$$Q_{MFVB} := \\{ q : q(\\theta) = \\prod_{j=1}^J \\: q_j(\\theta_j)\\}$$\n",
    "\n",
    "Ideally $\\theta$ factorizes into exponential distributions for whom it is easy to calculate *means* and *variances*.\n",
    "\n",
    "#### Optimization\n",
    "\n",
    "Now we apply optimization to find the optimal distribution $q^*$ which minimizes the $KL$ divergence between our appropriate distributions and the exact *posterior*. We are going to use *Stochastic Variational Inference (SVI)* for this purpose.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic computation with Pyro\n",
    "\n",
    "We will use [Pyro](https://pyro.ai/) for the probabilistic computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyro\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Use `Dataloader` to load the data and store it in the `data` directory.\n",
    "\n",
    "The dataset is split into two parts, training and testing.\n",
    "\n",
    "The testing sets is used to measure the model's performance on data it hasn't seen yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define how to transform the datasets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the datasets and where to store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "trainset = torchvision.datasets.MNIST('../data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "testset = torchvision.datasets.MNIST('../data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural net. It is identical to the classic approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = 28*28 # image size of 28 x 28 pixels\n",
    "hidden_layer_size = 1024 # number of hidden layer nodes\n",
    "output_layer_size = 10   # number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_layer, hidden_layer, output_layer):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_layer, hidden_layer)\n",
    "        self.out = nn.Linear(hidden_layer, output_layer)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instance new neural net for probabilistic calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baysNeuralNet = NeuralNet(input_layer_size, hidden_layer_size, output_layer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do Bayesian inference in Pyro we need a *model*, a *guide*, an *optimizer*, and an *inference algorithm*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "In general a *model* in `Pyro` is a Python function which uses `Pyro` primitives (= primitive stochastic functions) to compute the input data.\n",
    "\n",
    "One important thing to mention is that `pyro.sample` statements with an `obs` keyword are used to incorporate observations into the inference process. \n",
    "\n",
    "For Neural Networks we first need to convert our conventional network into a Bayesian network. Therefore we replace the fixed parameters for the weights and the biases by distributions. \n",
    "\n",
    "In our case we use the normal distribution to generate the priors.\n",
    "```python\n",
    "pyro.distributions.Normal()\n",
    "```\n",
    "After generating the priors, we replace the conventional, fixed parameters (weights and biases) by these new ones by calling \n",
    "```python\n",
    "pyro.random_module()\n",
    "```\n",
    "Now we can create a sampled neural network module with sampled weights and biases and pass in data to generate a result.\n",
    "\n",
    "In our case, the result will be 10 negative number representing the log probability of the 10 categories:\n",
    "\n",
    "<img src='../pictures/mode-raw-output.png' width=700 align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_data, y_data):\n",
    "    \n",
    "    # Initialize the prior distributions for the weights and biases of the neural network layers (N(0,1))\n",
    "    fc1_weight_prior = dist.Normal(loc=torch.zeros(baysNeuralNet.fc1.weight.size()), \n",
    "                        scale=torch.ones(baysNeuralNet.fc1.weight.size()))\n",
    "    fc1_bias_prior = dist.Normal(loc=torch.zeros(baysNeuralNet.fc1.bias.size()), \n",
    "                        scale=torch.ones(baysNeuralNet.fc1.bias.size()))\n",
    "\n",
    "    \n",
    "    out_weight_prior = dist.Normal(loc=torch.zeros(baysNeuralNet.out.weight.size()), \n",
    "                        scale=torch.ones(baysNeuralNet.out.weight.size()))\n",
    "    out_bias_prior = dist.Normal(loc=torch.zeros(baysNeuralNet.out.bias.size()), \n",
    "                        scale=torch.ones(baysNeuralNet.out.bias.size()))\n",
    "    \n",
    "    priors = {'fc1.weight': fc1_weight_prior, 'fc1.bias': fc1_bias_prior,  \n",
    "              'out.weight': out_weight_prior, 'out.bias': out_bias_prior}\n",
    "    \n",
    "    # place a prior over the parameters (weights and biases) of the network. \n",
    "    # Returns a distribution (callable) over nn.Module`s, which upon calling returns a sampled `nn.Module.\n",
    "    priored_module = pyro.random_module(\"module\", baysNeuralNet, priors)\n",
    "    \n",
    "    # get a sampled `nn.Module (callable) with sampled weights and biases \n",
    "    sampled_module = priored_module()\n",
    "    \n",
    "    # get the LogSoftmax result for each dataset (within a batch)\n",
    "    # each cathegory is represented by a negative number. (Eg 10 numbers for the 10 digits 0-9)\n",
    "    logProbPred = log_softmax(sampled_module(x_data))\n",
    "    \n",
    "    # incorporate observations into the model\n",
    "    # Call the stochastic function Categorical and return a sampled result according to the distribution if\n",
    "    # no obs parameter is set to condition on - otherwise use the obs argument to the pyro.sample statement \n",
    "    # to condition on the observed data y_data -> the result will by y_data\n",
    "    # Categorical creates a categorical distribution parameterized by logits (log_softmax())\n",
    "    pyro.sample(\"obs\", dist.Categorical(logits=logProbPred), obs=y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of pyro.sample **without** the observation argument will be stochastically sampled from `logProbPred` and will be different from `y_data`:\n",
    "<img src='../pictures/cat_sample_without_obs.png' width=800px>\n",
    "The result of pyro.sample **with** the observation argument will be identical to `y_data`:\n",
    "<img src='../pictures/cat_sample_with_obs.png' width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide\n",
    "The *guide* is a parameterized family of distributions over the *weights* and the *biases* and it is used to train the model by learning the parameters declared by `pyro.param` given the observation outcomes `obs`. `pyro.param` registers the parameters in the [ParamStore](http://docs.pyro.ai/en/0.2.1-release/parameters.html). \n",
    "\n",
    "The parameters to be learned are *fcl1_w_mu*, *fcl1_w_sigma*, *fcl1_b_mu*, *fcl1_b_sigma*, *outl_w_mu*, *outl_w_sigma*, *outl_b_mu* and *outl_b_sigma* and are defined within the *guide*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(x_data, y_data):\n",
    "    \n",
    "    # First layer weight distribution priors\n",
    "    fc1_weight_loc = torch.randn(baysNeuralNet.fc1.weight.size())\n",
    "    fc1_weight_scale = torch.randn(baysNeuralNet.fc1.weight.size())\n",
    "    fc1_weight_loc_param = pyro.param(\"fc1_weight_loc\", fc1_weight_loc)\n",
    "    fc1_weight_scale_param = softplus(pyro.param(\"fc1_weight_scale\", fc1_weight_scale))\n",
    "    fc1_weight_prior = dist.Normal(loc=fc1_weight_loc_param, scale=fc1_weight_scale_param)\n",
    "    \n",
    "    # First layer bias distribution priors\n",
    "    fc1_bias_loc = torch.randn(baysNeuralNet.fc1.bias.size())\n",
    "    fc1_bias_scale = torch.randn(baysNeuralNet.fc1.bias.size())\n",
    "    fc1_bias_loc_param = pyro.param(\"fc1_bias_loc\", fc1_bias_loc)\n",
    "    fc1_bias_scale_param = softplus(pyro.param(\"fc1_bias_scale\", fc1_bias_scale))\n",
    "    fc1_bias_prior = dist.Normal(loc=fc1_bias_loc_param, scale=fc1_bias_scale_param)\n",
    "    \n",
    "    # Output layer weight distribution priors\n",
    "    out_weight_loc = torch.randn(baysNeuralNet.out.weight.size())\n",
    "    out_weight_scale = torch.randn(baysNeuralNet.out.weight.size())\n",
    "    out_weight_loc_param = pyro.param(\"out_weight_loc\", out_weight_loc)\n",
    "    out_weight_scale_param = softplus(pyro.param(\"out_weight_scale\", out_weight_scale))\n",
    "    out_weight_prior = dist.Normal(loc=out_weight_loc_param, scale=out_weight_scale_param).independent(1)\n",
    "    \n",
    "    # Output layer bias distribution priors\n",
    "    out_bias_loc = torch.randn(baysNeuralNet.out.bias.size())\n",
    "    out_bias_scale = torch.randn(baysNeuralNet.out.bias.size())\n",
    "    out_bias_loc_param = pyro.param(\"out_bias_loc\", out_bias_loc)\n",
    "    out_bias_scale_param = softplus(pyro.param(\"out_bias_scale\", out_bias_scale))\n",
    "    out_bias_prior = dist.Normal(loc=out_bias_loc_param, scale=out_bias_scale_param)\n",
    "    \n",
    "    priors = {'fc1.weight': fc1_weight_prior, 'fc1.bias': fc1_bias_prior, \n",
    "              'out.weight': out_weight_prior, 'out.bias': out_bias_prior}\n",
    "    \n",
    "    priored_module = pyro.random_module(\"module\", baysNeuralNet, priors)\n",
    "    \n",
    "    return priored_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer & Inference Algorithm\n",
    "In this example we use the *Adam* optimizer and the *Stochastic Variational Inference (SVI)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.01})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Training is done by the *Stochastic Variational Inference (SVI)* optimization algorithm with the *ELBO* as the loss function (See the introductory part at the beginning of this notebook) by adjusting the `pyro.param` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/horst/anaconda3/envs/pyro/lib/python3.6/site-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
      "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  Loss  2106.8290526576043\n",
      "Epoch  2  Loss  378.6769510138989\n",
      "Epoch  3  Loss  161.66674293100039\n",
      "Epoch  4  Loss  112.00966752870877\n",
      "Epoch  5  Loss  97.43290654851596\n",
      "Epoch  6  Loss  90.94613541169167\n",
      "Epoch  7  Loss  87.42377650856972\n",
      "Epoch  8  Loss  86.76962037156423\n",
      "Epoch  9  Loss  86.66487224982579\n",
      "Epoch  10  Loss  85.86500588234266\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 10\n",
    "loss = 0\n",
    "\n",
    "for j in range(num_iterations):\n",
    "    loss = 0\n",
    "    for batch_id, data in enumerate(trainloader):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss += svi.step(data[0].view(-1,28*28), data[1])\n",
    "    normalizer_train = len(trainloader.dataset)\n",
    "    total_epoch_loss_train = loss / normalizer_train\n",
    "    \n",
    "    print(\"Epoch \", j+1, \" Loss \", total_epoch_loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Bayesian neural network\n",
    "In contrast to classical neural network with fixed parameters, Bayesian neural network with its random parameters need multiple runs / forwardpasses for a single input to predict an output. For an identical input file, each run will generate a different output. This is due to the randomness of the *weights* and *biases* of the Bayesian neural network. This is displayed in the following drawing from [Blundell et al](https://arxiv.org/pdf/1505.05424.pdf)\n",
    "<figure>\n",
    "    <img src='../pictures/Blundell_1.png' width=600 align=left />\n",
    "    <div style='width: 600px; text-align: center;'>Blundell et al.</div>\n",
    "</figure>\n",
    "\n",
    "On the left you can see a classical neural net with fixed *weights* and *biases* and on the right side you can see a Bayesian neural net with its random parameters.\n",
    "\n",
    "Therefore you need multiple runs to calculate the outcome.\n",
    "\n",
    "In the implementation below we use the *guide()* for prediction because it has the trained parameters.\n",
    "1. The first statement within *prediction()* generates *num_of_samples* Bayesian networks\n",
    "2. The second statement generates *num_of_samples* predictions for each single input. Thus if we have a test batch size of 64 we get in total 640 predictions, 10 for each input.\n",
    "3. The forth statement calculates the mean / average over the *num_of_samples* output for each input and each category. Thus yielding in 64 tensors with a mean value for each category.\n",
    "4. The result is the index of the category with the maximal mean value. There is one value for each input.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction when network is forced to predict\n",
      "Accuracy: 89.9 %\n"
     ]
    }
   ],
   "source": [
    "# number of multiple runs for a single input\n",
    "num_of_samples = 10\n",
    "\n",
    "def predict(x):\n",
    "    # generate num_of_samples models \n",
    "    sampled_models = [guide(None, None) for _ in range(num_of_samples)]\n",
    "    # forwardpass of the date -> generates num_of_samples predictions per input\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    # stack the tensors for further processing\n",
    "    inter = torch.stack(yhats)\n",
    "    # Build the mean value for each category out of the sampled models\n",
    "    mean = torch.mean(inter, 0)\n",
    "    # Build the variance for each category out of the sampled models\n",
    "    variance = torch.var(inter, 0)\n",
    "    # get the category with the maximal mean value\n",
    "    # this is the predicted value of the Bayesian natwork\n",
    "    res = np.argmax(mean.numpy(), axis=1)\n",
    "    return res\n",
    "\n",
    "print('Prediction when network is forced to predict')\n",
    "correct = 0\n",
    "total = 0\n",
    "for j, data in enumerate(testloader):\n",
    "    images, labels = data\n",
    "    with torch.no_grad():\n",
    "        predicted = predict(images.view(-1,28*28))\n",
    "    total += labels.size(0)\n",
    "    correct += np.equal(predicted, labels).sum()\n",
    "print(\"Accuracy: {:.1f} %\".format(correct / total * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    #plt.imshow(npimg,  cmap='gray')\n",
    "    #fig.show(figsize=(1,1))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(1, 1))\n",
    "    ax.imshow(npimg,  cmap='gray', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of multiple runs for a single input\n",
    "num_of_samples = 100\n",
    "\n",
    "def give_uncertainities(x):\n",
    "    # generate num_of_samples models \n",
    "    sampled_models = [guide(None, None) for _ in range(num_of_samples)]\n",
    "#     print('---sampled_models', sampled_models)\n",
    "    \n",
    "    yhats = [F.log_softmax(model(x.view(-1,28*28)).data, 1).numpy() for model in sampled_models]\n",
    "#     print('\\n\\n---yhats---\\n', yhats)\n",
    "#     print(type(yhats))\n",
    "    \n",
    "    ret = np.asarray(yhats)\n",
    "#     print('\\n\\n---ret---\\n', ret)\n",
    "    \n",
    "    a = np.stack(ret, 1)\n",
    "#     print('\\n\\n---a---\\n', a)\n",
    "    \n",
    "    b = np.var(a, 1)\n",
    "    print('\\n\\n---b---\\n', b)\n",
    "    \n",
    "    c = np.mean(b, 1)\n",
    "    print('\\n\\n---c---\\n', c)\n",
    "    \n",
    "    d = []\n",
    "    for i in range(len(c)):\n",
    "        d.append(b[i] / c[i])\n",
    "        \n",
    "    e = np.array(d)\n",
    "    \n",
    "    print('\\n\\n---e---\\n', e)\n",
    "    \n",
    "    f = np.min(e, 1)\n",
    "    print('\\n\\n---f---\\n', f)\n",
    "    f1 = np.argmin(e, 1)\n",
    "    print('\\n\\n---f1---\\n', f1)\n",
    "    \n",
    "    for i in range(len(f)):\n",
    "        if f[i] < 0.20:\n",
    "            print('Predicted: ', f1[i])\n",
    "        else:\n",
    "            print('not sure!')\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---b---\n",
      " [[1.1616863e+04 1.4544838e+04 8.4521777e+03 1.1399032e+04 1.2271894e+04\n",
      "  1.0597495e+04 1.3206329e+04 1.4068765e-16 9.3657285e+03 9.3557432e+03]\n",
      " [1.2491934e+04 1.2904848e+04 1.3804742e+03 1.4408510e+04 1.4578821e+04\n",
      "  1.2460544e+04 1.2242878e+04 1.4699214e+04 1.2978215e+04 1.4790630e+04]\n",
      " [8.1835288e+03 3.7404198e+01 6.4974839e+03 7.1210020e+03 3.8370615e+03\n",
      "  6.3026777e+03 6.1932383e+03 6.3524688e+03 6.6258979e+03 5.7821143e+03]]\n",
      "\n",
      "\n",
      "---c---\n",
      " [10081.01   12293.607   5693.2876]\n",
      "\n",
      "\n",
      "---e---\n",
      " [[1.1523511e+00 1.4427958e+00 8.3842570e-01 1.1307431e+00 1.2173278e+00\n",
      "  1.0512335e+00 1.3100204e+00 1.3955710e-20 9.2904669e-01 9.2805618e-01]\n",
      " [1.0161325e+00 1.0497202e+00 1.1229204e-01 1.1720327e+00 1.1858864e+00\n",
      "  1.0135791e+00 9.9587351e-01 1.1956794e+00 1.0556880e+00 1.2031155e+00]\n",
      " [1.4373995e+00 6.5698768e-03 1.1412534e+00 1.2507715e+00 6.7396235e-01\n",
      "  1.1070366e+00 1.0878141e+00 1.1157821e+00 1.1638087e+00 1.0156020e+00]]\n",
      "\n",
      "\n",
      "---f---\n",
      " [1.3955710e-20 1.1229204e-01 6.5698768e-03]\n",
      "\n",
      "\n",
      "---f1---\n",
      " [7 2 1]\n",
      "Predicted:  7\n",
      "Predicted:  2\n",
      "Predicted:  1\n",
      "Real:  7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFoAAABYCAYAAAB1YOAJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAE5klEQVR4nO2cX0idZRzHP9/MLrQukmZKiUZMNDZUkBAc2IVByGB50WhCNAjWRUpBF42uugypoKvAaFASZFijjaFdjNrucjalWiMZIeWSbYHQUiTm+XVxXp3OP+d4/vzOOa/PBw7nfd9z3vf58j1ffud5n/OcR2ZGIP/cV2gBe4VgtBPBaCeC0U4Eo50IRjuRldGSnpP0m6Rrkk7mSlQcUab9aEllwAzwLDAHXAKOmdmvuZMXH+7P4tyngWtm9juApC+AI8C2RkuK7d2RmWmn17MpHY8Bf67bn4uObUDSCUmTkiazaKvkySbRW32CmxJrZkPAEMQ70anIJtFzQN26/ceBv7KTE1+yMfoSsF/SE5IeAF4EzuRGVvzIuHSY2R1J/cC3QBlwysyu5ExZzMi4e5dRYzGu0fnsdQR2wZ4xurGxkcbGRhKJBIlEgoGBAQYGBtza3zNGF5ps+tElRVtbGwCJRAKAubk51/ZDop3YM4lubW0FYHFxEYDTp0+7th8S7UTsE33gwAEA+vv7ARgeHi6IjpBoJ2Kf6KamJgAqKysBGBkZKYiOkGgnYj/WMTExAcC+ffuAuzV7tfeRK8JYR5EQ2xrd0NAAQHt7OwAzMzNA7pOcLiHRTsQ20V1dXRv2b926VSAlSUKinYhtog8ePLhhf3BwsEBKkoREOxHLfnRHRwfnzp0DYHZ2FoDOzk4AlpeX89Jmqn50LEtHd3c3VVVVAIyPjwP5MzhdQulwIpaJbmlpYbUkjo6OFlhNkpBoJ2L1ZVhTUwPA9PQ0CwsLADQ3N+ezyTXCoFKREKsaffz4cQCqq6sZGxsrrJh7CIl2IlaJrq+vX9terdHFQki0EykTLakO+AyoARLAkJl9KKkKGAEagFngqJkVNEaHDx9e2z579mwBlWwmnUTfAd40s2agA3hN0lPASeC8me0Hzkf7gW1ImWgzmwfmo+3bkq6S/PfVEeCZ6G2fAt8Db+VFZQoOHToE3O1HFyO7+jKU1AC0AT8Aj0YfAmY2L6l6m3NOACey1FnypG20pAeBr4A3zOwfaccboTU8/v7W29sLQFlZGQBTU1NcvHgxH01lTFq9DknlJE3+3My+jg7fkFQbvV4L3MyPxHiQTq9DwCfAVTP7YN1LZ4CXgXej52/yonAHKioqAOjp6dlwfHR0lJWVFW85O5JO6egEXgJ+ljQdHXubpMFfSnoF+AN4IS8KY0JJj96Vl5cDcOHCBQBu3kxWr76+PpaWlnLZVErC6F2RUNKJLiZCoouEYLQTwWgngtFOBKOdCEY7EYx2wvs3w7+Bxei5VHmEzfrrt3rjelxvWAAkTZpZu2ujOSRT/aF0OBGMdqIQRg8VoM1ckpF+9xq9Vwmlw4lgtBNuRpfiot6S6iR9J+mqpCuSXo+OvyPpuqTp6NGT8loeNbpUF/WOft2vNbPLkh4CfgSeB44C/5rZe+leyyvRa4t6m9l/wOqi3kWNmc2b2eVo+zawOktr13gZndai3sXMPbO0APol/STplKSHU53vZXRai3oXK/fO0gI+Ap4EWknOS3w/1TW8jC7ZRb23mqVlZjfMbMXMEsDHJEvjjngZXZKLem83S2t1KlxEL/BLqmu5DJOW8KLe283SOiaplWT5mwVeTXWhcAvuRLgzdCIY7UQw2olgtBPBaCeC0U4Eo534H449kgJxtnLPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "Total images:  3\n",
      "Predicted for:  3\n",
      "Accuracy when predicted:  1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 3.0, 3)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "test_batch(images[:3], labels[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "\n",
    "def test_batch(images, labels, plot=True):\n",
    "    y = give_uncertainities(images)\n",
    "    predicted_for_images = 0\n",
    "    correct_predictions=0\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "    \n",
    "        if(plot):\n",
    "            print(\"Real: \",labels[i].item())\n",
    "            fig, axs = plt.subplots(1, 10, sharey=True,figsize=(20,2))\n",
    "    \n",
    "        all_digits_prob = []\n",
    "    \n",
    "        highted_something = False\n",
    "    \n",
    "        for j in range(len(classes)):\n",
    "        \n",
    "            highlight=False\n",
    "        \n",
    "            histo = []\n",
    "            histo_exp = []\n",
    "        \n",
    "            for z in range(y.shape[0]):\n",
    "                histo.append(y[z][i][j])\n",
    "                histo_exp.append(np.exp(y[z][i][j]))\n",
    "            \n",
    "            prob = np.percentile(histo_exp, 50) #sampling median probability\n",
    "        \n",
    "            if(prob>0.2): #select if network thinks this sample is 20% chance of this being a label\n",
    "                highlight = True #possibly an answer\n",
    "        \n",
    "            all_digits_prob.append(prob)\n",
    "            \n",
    "            if(plot):\n",
    "            \n",
    "                N, bins, patches = axs[j].hist(histo, bins=8, color = \"lightgray\", lw=0,  weights=np.ones(len(histo)) / len(histo), density=False)\n",
    "                axs[j].set_title(str(j)+\" (\"+str(round(prob,2))+\")\") \n",
    "        \n",
    "            if(highlight):\n",
    "            \n",
    "                highted_something = True\n",
    "                \n",
    "                if(plot):\n",
    "\n",
    "                    # We'll color code by height, but you could use any scalar\n",
    "                    fracs = N / N.max()\n",
    "\n",
    "                    # we need to normalize the data to 0..1 for the full range of the colormap\n",
    "                    norm = colors.Normalize(fracs.min(), fracs.max())\n",
    "\n",
    "                    # Now, we'll loop through our objects and set the color of each accordingly\n",
    "                    for thisfrac, thispatch in zip(fracs, patches):\n",
    "                        color = plt.cm.viridis(norm(thisfrac))\n",
    "                        thispatch.set_facecolor(color)\n",
    "\n",
    "    \n",
    "        if(plot):\n",
    "            plt.show()\n",
    "    \n",
    "        predicted = np.argmax(all_digits_prob)\n",
    "    \n",
    "        if(highted_something):\n",
    "            predicted_for_images+=1\n",
    "            if(labels[i].item()==predicted):\n",
    "                if(plot):\n",
    "                    print(\"Correct\")\n",
    "                correct_predictions +=1.0\n",
    "            else:\n",
    "                if(plot):\n",
    "                    print(\"Incorrect :()\")\n",
    "        else:\n",
    "            if(plot):\n",
    "                print(\"Undecided.\")\n",
    "        \n",
    "        if(plot):\n",
    "            imshow(images[i].squeeze())\n",
    "        \n",
    "    \n",
    "    if(plot):\n",
    "        print(\"Summary\")\n",
    "        print(\"Total images: \",len(labels))\n",
    "        print(\"Predicted for: \",predicted_for_images)\n",
    "        print(\"Accuracy when predicted: \",correct_predictions/predicted_for_images)\n",
    "        \n",
    "    return len(labels), correct_predictions, predicted_for_images "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Prediction when network can decide not to predict\n",
    "\n",
    "print('Prediction when network can refuse')\n",
    "correct = 0\n",
    "total = 0\n",
    "total_predicted_for = 0\n",
    "for j, data in enumerate(testloader):\n",
    "    images, labels = data\n",
    "    \n",
    "    total_minibatch, correct_minibatch, predictions_minibatch = test_batch(images, labels, plot=False)\n",
    "    total += total_minibatch\n",
    "    correct += correct_minibatch\n",
    "    total_predicted_for += predictions_minibatch\n",
    "\n",
    "print(\"Total images: \", total)\n",
    "print(\"Skipped: \", total-total_predicted_for)\n",
    "print(\"Accuracy when made predictions: %d %%\" % (100 * correct / total_predicted_for))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
