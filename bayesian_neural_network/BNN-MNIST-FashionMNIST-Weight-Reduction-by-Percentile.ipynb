{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence of a Bayesian Neural Network\n",
    "This notebook examines the confidence calculation of a *Bayesian* neural network. I will use the *MNIST* and *FashionMNIST* datasets to demonstrate how confidence within the prediction process can help to avoid misclassification. `PyTorch` and `Pyro` are used for the probabilistic programming part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic computation with Pyro\n",
    "\n",
    "We will use [Pyro](https://pyro.ai/) for the probabilistic computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import time\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.distributions import constraints\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyro\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide whether to use trained and saved parameter or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_saved_params = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the percentile value to determine the amount of parameters whose values should be substituted by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_val = 88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Pyro version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the device. It is either cuda:0 or cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Use `Dataloader` to load the data and store it in the `data` directory.\n",
    "\n",
    "The dataset is split into two parts, training and testing.\n",
    "\n",
    "The testing sets is used to measure the model's performance on data it hasn't seen yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define how to transform the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the datasets and where to store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "trainset = torchvision.datasets.MNIST('../data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "testset = torchvision.datasets.MNIST('../data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural net. It is identical to the classic approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = 28*28 # image size of 28 x 28 pixels\n",
    "hidden_layer_size = 1024 # number of hidden layer nodes\n",
    "output_layer_size = 10   # number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_layer, hidden_layer, output_layer):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_layer, hidden_layer)\n",
    "        self.out = nn.Linear(hidden_layer, output_layer)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instance new neural network for probabilistic calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "baysNeuralNet = NeuralNet(input_layer_size, hidden_layer_size, output_layer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the network to `cuda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
       "  (out): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baysNeuralNet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do Bayesian inference in Pyro we need a *model*, a *guide*, an *optimizer*, and an *inference algorithm*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "In general a *model* in `Pyro` is a Python function which uses `Pyro` primitives (= primitive stochastic functions) to compute the input data.\n",
    "\n",
    "One important thing to mention is that `pyro.sample` statements with an `obs` keyword are used to incorporate observations into the inference process. \n",
    "\n",
    "For Neural Networks we first need to convert our conventional network into a Bayesian network. Therefore we replace the fixed parameters for the weights and the biases by distributions. \n",
    "\n",
    "In our case we use the normal distribution to generate the priors.\n",
    "```python\n",
    "pyro.distributions.Normal()\n",
    "```\n",
    "After generating the priors, we replace the conventional, fixed parameters (weights and biases) by these new ones by calling \n",
    "```python\n",
    "pyro.random_module()\n",
    "```\n",
    "Now we can create a sampled neural network module with sampled weights and biases and pass in data to generate a result.\n",
    "\n",
    "In our case, the result will be 10 negative number representing the log probability of the 10 categories:\n",
    "\n",
    "<img src='../pictures/mode-raw-output.png' width=700 align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_data, y_data):\n",
    "    \n",
    "    # Initialize the prior distributions for the weights and biases of the neural network layers (N(0,1))\n",
    "    fc1_weight_prior = dist.Normal(loc=torch.zeros(baysNeuralNet.fc1.weight.size()).to(device), \n",
    "                        scale=torch.ones(baysNeuralNet.fc1.weight.size()).to(device))\n",
    "    fc1_bias_prior = dist.Normal(loc=torch.zeros(baysNeuralNet.fc1.bias.size()).to(device), \n",
    "                        scale=torch.ones(baysNeuralNet.fc1.bias.size()).to(device))\n",
    "\n",
    "    \n",
    "    out_weight_prior = dist.Normal(loc=torch.zeros(baysNeuralNet.out.weight.size()).to(device), \n",
    "                        scale=torch.ones(baysNeuralNet.out.weight.size()).to(device))\n",
    "    out_bias_prior = dist.Normal(loc=torch.zeros(baysNeuralNet.out.bias.size()).to(device), \n",
    "                        scale=torch.ones(baysNeuralNet.out.bias.size()).to(device))\n",
    "    \n",
    "    priors = {'fc1.weight': fc1_weight_prior, 'fc1.bias': fc1_bias_prior,  \n",
    "              'out.weight': out_weight_prior, 'out.bias': out_bias_prior}\n",
    "    \n",
    "    # place a prior over the parameters (weights and biases) of the network. \n",
    "    # Returns a distribution (callable) over nn.Module`s, which upon calling returns a sampled `nn.Module.\n",
    "    priored_module = pyro.random_module(\"module\", baysNeuralNet, priors)\n",
    "    \n",
    "    # get a sampled `nn.Module (callable) with sampled weights and biases \n",
    "    sampled_module = priored_module()\n",
    "    \n",
    "    # get the LogSoftmax result for each dataset (within a batch)\n",
    "    # each cathegory is represented by a negative number. (Eg 10 numbers for the 10 digits 0-9)\n",
    "    logProbPred = log_softmax(sampled_module(x_data))\n",
    "    \n",
    "    # incorporate observations into the model\n",
    "    # Call the stochastic function Categorical and return a sampled result according to the distribution if\n",
    "    # no obs parameter is set to condition on - otherwise use the obs argument to the pyro.sample statement \n",
    "    # to condition on the observed data y_data -> the result will by y_data\n",
    "    # Categorical creates a categorical distribution parameterized by logits (log_softmax())\n",
    "    pyro.sample(\"obs\", dist.Categorical(logits=logProbPred), obs=y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of pyro.sample **without** the observation argument will be stochastically sampled from `logProbPred` and will be different from `y_data`:\n",
    "<img src='../pictures/cat_sample_without_obs.png' width=800px>\n",
    "The result of pyro.sample **with** the observation argument will be identical to `y_data`:\n",
    "<img src='../pictures/cat_sample_with_obs.png' width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide\n",
    "The *guide* is a parameterized family of distributions over the *weights* and the *biases* and it is used to train the model by learning the parameters declared by `pyro.param` given the observation outcomes `obs`. `pyro.param` registers the parameters in the [ParamStore](http://docs.pyro.ai/en/0.2.1-release/parameters.html). \n",
    "\n",
    "The parameters to be learned are *fcl1_w_mu*, *fcl1_w_sigma*, *fcl1_b_mu*, *fcl1_b_sigma*, *outl_w_mu*, *outl_w_sigma*, *outl_b_mu* and *outl_b_sigma* and are defined within the *guide*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(x_data, y_data):\n",
    "    \n",
    "    # First layer weight distribution priors\n",
    "    fc1_weight_loc = torch.randn(baysNeuralNet.fc1.weight.size()).to(device)\n",
    "    fc1_weight_scale = torch.rand(baysNeuralNet.fc1.weight.size()).to(device)\n",
    "    fc1_weight_loc_param = pyro.param(\"fc1_weight_loc\", fc1_weight_loc)\n",
    "    fc1_weight_scale_param = softplus(pyro.param(\"fc1_weight_scale\", fc1_weight_scale, constraint=constraints.positive))\n",
    "    fc1_weight_prior = dist.Normal(loc=fc1_weight_loc_param, scale=fc1_weight_scale_param)\n",
    "    \n",
    "    # First layer bias distribution priors\n",
    "    fc1_bias_loc = torch.randn(baysNeuralNet.fc1.bias.size()).to(device)\n",
    "    fc1_bias_scale = torch.rand(baysNeuralNet.fc1.bias.size()).to(device)\n",
    "    fc1_bias_loc_param = pyro.param(\"fc1_bias_loc\", fc1_bias_loc)\n",
    "    fc1_bias_scale_param = softplus(pyro.param(\"fc1_bias_scale\", fc1_bias_scale, constraint=constraints.positive))\n",
    "    fc1_bias_prior = dist.Normal(loc=fc1_bias_loc_param, scale=fc1_bias_scale_param)\n",
    "    \n",
    "    # Output layer weight distribution priors\n",
    "    out_weight_loc = torch.randn(baysNeuralNet.out.weight.size()).to(device)\n",
    "    out_weight_scale = torch.rand(baysNeuralNet.out.weight.size()).to(device)\n",
    "    out_weight_loc_param = pyro.param(\"out_weight_loc\", out_weight_loc)\n",
    "    out_weight_scale_param = softplus(pyro.param(\"out_weight_scale\", out_weight_scale, constraint=constraints.positive))\n",
    "    out_weight_prior = dist.Normal(loc=out_weight_loc_param, scale=out_weight_scale_param).independent(1)\n",
    "    \n",
    "    # Output layer bias distribution priors\n",
    "    out_bias_loc = torch.randn(baysNeuralNet.out.bias.size()).to(device)\n",
    "    out_bias_scale = torch.rand(baysNeuralNet.out.bias.size()).to(device)\n",
    "    out_bias_loc_param = pyro.param(\"out_bias_loc\", out_bias_loc)\n",
    "    out_bias_scale_param = softplus(pyro.param(\"out_bias_scale\", out_bias_scale, constraint=constraints.positive))\n",
    "    out_bias_prior = dist.Normal(loc=out_bias_loc_param, scale=out_bias_scale_param)\n",
    "    \n",
    "    priors = {'fc1.weight': fc1_weight_prior, 'fc1.bias': fc1_bias_prior, \n",
    "              'out.weight': out_weight_prior, 'out.bias': out_bias_prior}\n",
    "    \n",
    "    priored_module = pyro.random_module(\"module\", baysNeuralNet, priors)\n",
    "    \n",
    "    return priored_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer & Inference Algorithm\n",
    "In this example we use the *Adam* optimizer and the *Stochastic Variational Inference (SVI)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.01})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Training is done by the *Stochastic Variational Inference (SVI)* optimization algorithm with the *ELBO* as the loss function (See the introductory part at the beginning of this notebook) by adjusting the `pyro.param` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_saved_params == False:\n",
    "\n",
    "    num_iterations = 10\n",
    "    loss = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for j in range(num_iterations):\n",
    "        loss = 0\n",
    "        for batch_id, (images, labels) in enumerate(trainloader):\n",
    "            # calculate the loss and take a gradient step\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            loss += svi.step(images.view(-1,28*28), labels)\n",
    "\n",
    "        normalizer_train = len(trainloader.dataset)\n",
    "        total_epoch_loss_train = loss / normalizer_train\n",
    "\n",
    "        print(\"Epoch \", j+1, \" Loss \", total_epoch_loss_train)\n",
    "\n",
    "    print(\"\\nTime for training: {:.0f} minutes and {:.3f} seconds\"\n",
    "          .format((time.time() - start)/60, (time.time() - start) % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save trained parameters to disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_saved_params == False:\n",
    "    pyro.get_param_store().save('saved_params.save')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_saved_params == True:\n",
    "    pyro.get_param_store().load('saved_params.save')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the trained parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the parameter names and check their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1_weight_loc 802816\n",
      "fc1_weight_scale 802816\n",
      "fc1_bias_loc 1024\n",
      "fc1_bias_scale 1024\n",
      "out_weight_loc 10240\n",
      "out_weight_scale 10240\n",
      "out_bias_loc 10\n",
      "out_bias_scale 10\n"
     ]
    }
   ],
   "source": [
    "for name in pyro.get_param_store().get_all_param_names():\n",
    "    print(name, pyro.param(name).cpu().detach().numpy().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shape of the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1_weight_loc (1024, 784)\n",
      "fc1_weight_scale (1024, 784)\n",
      "fc1_bias_loc (1024,)\n",
      "fc1_bias_scale (1024,)\n",
      "out_weight_loc (10, 1024)\n",
      "out_weight_scale (10, 1024)\n",
      "out_bias_loc (10,)\n",
      "out_bias_scale (10,)\n"
     ]
    }
   ],
   "source": [
    "for name in pyro.get_param_store().get_all_param_names():\n",
    "    print(name, pyro.param(name).cpu().detach().numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the first value of each parameter type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First value of parameter fc1_weight_loc is -0.0892615020275116\n",
      "First value of parameter fc1_weight_scale is 0.464173823595047\n",
      "First value of parameter fc1_bias_loc is -0.39173540472984314\n",
      "First value of parameter fc1_bias_scale is 0.4639877378940582\n",
      "First value of parameter out_weight_loc is -0.012034603394567966\n",
      "First value of parameter out_weight_scale is 0.2915618419647217\n",
      "First value of parameter out_bias_loc is -0.11939097195863724\n",
      "First value of parameter out_bias_scale is 0.44849273562431335\n"
     ]
    }
   ],
   "source": [
    "for name in pyro.get_param_store().get_all_param_names():\n",
    "    if len(pyro.param(name).cpu().detach().numpy().shape) == 1:\n",
    "#         print('Size for parameter {} is {}'. format(name, pyro.param(name).cpu().detach().numpy().size))\n",
    "        print('First value of parameter {} is {}'. format(name, pyro.param(name).cpu().detach().numpy()[0]))\n",
    "    elif len(pyro.param(name).cpu().detach().numpy().shape) == 2:\n",
    "#         print('Size for parameter {} is {}'. format(name, pyro.param(name).cpu().detach().numpy().size))\n",
    "        print('First value of parameter {} is {}'. format(name, pyro.param(name).cpu().detach().numpy()[0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the learned parameters for further processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_weight_loc_temp = pyro.param('fc1_weight_loc').cpu().detach().numpy()\n",
    "fc1_weight_scale_temp = pyro.param('fc1_weight_scale').cpu().detach().numpy()\n",
    "fc1_bias_loc_temp = pyro.param('fc1_bias_loc').cpu().detach().numpy()\n",
    "fc1_bias_scale_temp = pyro.param('fc1_bias_scale').cpu().detach().numpy()\n",
    "out_weight_loc_temp = pyro.param('out_weight_loc').cpu().detach().numpy()\n",
    "out_weight_scale_temp = pyro.param('out_weight_scale').cpu().detach().numpy()\n",
    "out_bias_loc_temp = pyro.param('out_bias_loc').cpu().detach().numpy()\n",
    "out_bias_scale_temp = pyro.param('out_bias_scale').cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check minimum variance value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008764488\n"
     ]
    }
   ],
   "source": [
    "print(np.min(fc1_weight_scale_temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal-to-noise ratio\n",
    "Calculate the signal to noise ratio for the fc1 weights according to [Blundell et al](https://arxiv.org/pdf/1505.05424.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0145854e-07\n",
      "37.155296\n"
     ]
    }
   ],
   "source": [
    "fc1_weight_signal_noise_ratio = np.abs(fc1_weight_loc_temp) / fc1_weight_scale_temp\n",
    "fc1_bias_signal_noise_ratio = np.abs(fc1_bias_loc_temp) / fc1_bias_scale_temp\n",
    "out_weight_signal_noise_ratio = np.abs(out_weight_loc_temp) / out_weight_scale_temp\n",
    "out_bias_signal_noise_ration = np.abs(out_bias_loc_temp) / out_bias_scale_temp\n",
    "# print(fc1_weight_signal_noise_ratio)\n",
    "# print(fc1_weight_signal_noise_ratio.shape)\n",
    "print(np.min(fc1_weight_signal_noise_ratio))\n",
    "print(np.max(fc1_weight_signal_noise_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.ravel(fc1_weight_signal_noise_ratio)\n",
    "plt.xlim(-0.5, 1.0)\n",
    "sns.kdeplot(x = a, shade=\"fill\")\n",
    "\n",
    "# Plot formatting\n",
    "plt.title('Density Plot')\n",
    "plt.xlabel('Signal-to-Noise ratio (dB)')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean for the signal to noise ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1671208\n"
     ]
    }
   ],
   "source": [
    "fc1_weight_mean = np.mean(fc1_weight_signal_noise_ratio)\n",
    "fc1_bias_mean = np.mean(fc1_bias_signal_noise_ratio)\n",
    "out_weight_mean = np.mean(out_weight_signal_noise_ratio)\n",
    "out_bias_mean = np.mean(out_bias_signal_noise_ration)\n",
    "print(fc1_weight_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the percentile for the signal to noise ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88th percentile of fc1 weights: 0.32299870252609236\n"
     ]
    }
   ],
   "source": [
    "fc1_weight_percentile = np.percentile(fc1_weight_signal_noise_ratio, percentile_val)\n",
    "fc1_bias_percentile = np.percentile(fc1_bias_signal_noise_ratio, percentile_val)\n",
    "out_weight_percentile = np.percentile(out_weight_signal_noise_ratio, percentile_val)\n",
    "out_bias_percentile = np.percentile(out_bias_signal_noise_ration, percentile_val)\n",
    "print('{}th percentile of fc1 weights: {}'.format(percentile_val, fc1_weight_percentile))\n",
    "# np.savetxt('results.csv', [percentile_val], delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the indexes of all signal to noise rations less than the mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of \"deleted\" weights between input and hidden layer: 706478 = 88.0 %\n",
      "# of \"deleted\" biases between input and hidden layer: 901 = 88.0 %\n",
      "# of \"deleted\" weights between hidden and output layer: 9011 = 88.0 %\n",
      "# of \"deleted\" bias between hidden and output layer: 8 = 80.0 %\n"
     ]
    }
   ],
   "source": [
    "fc1_weight_signal_noise_ratio_index = np.argwhere(fc1_weight_signal_noise_ratio < fc1_weight_percentile)\n",
    "fc1_bias_signal_noise_ratio_index = np.argwhere(fc1_bias_signal_noise_ratio < fc1_bias_percentile)\n",
    "out_weight_signal_noise_ratio_index = np.argwhere(out_weight_signal_noise_ratio < out_weight_percentile)\n",
    "out_bias_signal_noise_ratio_index = np.argwhere(out_bias_signal_noise_ration < out_bias_percentile)\n",
    "\n",
    "print('# of \"deleted\" weights between input and hidden layer: {} = {:.3} %'. \n",
    "      format(len(fc1_weight_signal_noise_ratio_index), \n",
    "             (len(fc1_weight_signal_noise_ratio_index) / \n",
    "              pyro.param('fc1_weight_loc').cpu().detach().numpy().size) * 100))\n",
    "\n",
    "# np.savetxt('results.csv', (len(fc1_weight_signal_noise_ratio_index), (len(fc1_weight_signal_noise_ratio_index) / pyro.param('fc1_weight_loc').cpu().detach().numpy().size) * 100), delimiter = ',')\n",
    "\n",
    "print('# of \"deleted\" biases between input and hidden layer: {} = {:.3} %'. \n",
    "      format(len(fc1_bias_signal_noise_ratio_index), \n",
    "             (len(fc1_bias_signal_noise_ratio_index) / \n",
    "              pyro.param('fc1_bias_loc').cpu().detach().numpy().size) * 100))\n",
    "print('# of \"deleted\" weights between hidden and output layer: {} = {:.3} %'. \n",
    "      format(len(out_weight_signal_noise_ratio_index), \n",
    "             (len(out_weight_signal_noise_ratio_index) / \n",
    "              pyro.param('out_weight_loc').cpu().detach().numpy().size) * 100))\n",
    "print('# of \"deleted\" bias between hidden and output layer: {} = {:.3} %'. \n",
    "      format(len(out_bias_signal_noise_ratio_index), \n",
    "             (len(out_bias_signal_noise_ratio_index) / \n",
    "              pyro.param('out_bias_loc').cpu().detach().numpy().size) * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all *mean* and *variance* values that are less than the calculated mean value by zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fc1_weight_signal_noise_ratio_index)):\n",
    "    fc1_weight_loc_temp[fc1_weight_signal_noise_ratio_index[i, 0], \n",
    "                     fc1_weight_signal_noise_ratio_index[i, 1]] = 0\n",
    "    fc1_weight_scale_temp[fc1_weight_signal_noise_ratio_index[i, 0], \n",
    "                     fc1_weight_signal_noise_ratio_index[i, 1]] = 0\n",
    "#     print(fc1_weight_signal_noise_ratio_mean_index[i, 0], fc1_weight_signal_noise_ratio_mean_index[i, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fc1_bias_signal_noise_ratio_index)):\n",
    "    fc1_bias_loc_temp[fc1_bias_signal_noise_ratio_index[i, 0]] = 0\n",
    "    fc1_bias_scale_temp[fc1_bias_signal_noise_ratio_index[i, 0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(out_weight_signal_noise_ratio_index)):\n",
    "    out_weight_loc_temp[out_weight_signal_noise_ratio_index[i, 0], \n",
    "                     out_weight_signal_noise_ratio_index[i, 1]] = 0\n",
    "    out_weight_scale_temp[out_weight_signal_noise_ratio_index[i, 0], \n",
    "                     out_weight_signal_noise_ratio_index[i, 1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(out_bias_signal_noise_ratio_index)):\n",
    "    out_bias_loc_temp[out_bias_signal_noise_ratio_index[i, 0]] = 0\n",
    "    out_bias_scale_temp[out_bias_signal_noise_ratio_index[i, 0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_weight_loc_zeros = torch.from_numpy(fc1_weight_loc_temp)\n",
    "fc1_weight_scale_zeros = torch.from_numpy(fc1_weight_scale_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_bias_loc_zeros = torch.from_numpy(fc1_bias_loc_temp)\n",
    "fc1_bias_scale_zeros = torch.from_numpy(fc1_bias_scale_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_weight_loc_zeros = torch.from_numpy(out_weight_loc_temp)\n",
    "out_weight_scale_zeros = torch.from_numpy(out_weight_scale_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_bias_loc_zeros = torch.from_numpy(out_bias_loc_temp)\n",
    "out_bias_scale_zeros = torch.from_numpy(out_bias_scale_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update parameter score\n",
    "1. Clear parameter score\n",
    "2. Update altered parameters\n",
    "3. Reset unchanged parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear parameter score\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# update changed parameters\n",
    "fc1_weight_loc = fc1_weight_loc_zeros.to(device)\n",
    "fc1_weight_scale = fc1_weight_scale_zeros.to(device)\n",
    "fc1_weight_loc_param = pyro.param(\"fc1_weight_loc\", fc1_weight_loc)\n",
    "fc1_weight_scale_param = softplus(pyro.param(\"fc1_weight_scale\", fc1_weight_scale))\n",
    "\n",
    "# restore unchanged parameter\n",
    "fc1_bias_loc = fc1_bias_loc_zeros.to(device)\n",
    "fc1_bias_scale = fc1_bias_scale_zeros.to(device)\n",
    "fc1_bias_loc_param = pyro.param(\"fc1_bias_loc\", fc1_bias_loc)\n",
    "fc1_bias_scale_param = softplus(pyro.param(\"fc1_bias_scale\", fc1_bias_scale))\n",
    "\n",
    "out_weight_loc = out_weight_loc_zeros.to(device)\n",
    "out_weight_scale = out_weight_scale_zeros.to(device)\n",
    "out_weight_loc_param = pyro.param(\"out_weight_loc\", out_weight_loc)\n",
    "out_weight_scale_param = softplus(pyro.param(\"out_weight_scale\", out_weight_scale))\n",
    "\n",
    "out_bias_loc = out_bias_loc_zeros.to(device)\n",
    "out_bias_scale = out_bias_scale_zeros.to(device)\n",
    "out_bias_loc_param = pyro.param(\"out_bias_loc\", out_bias_loc)\n",
    "out_bias_scale_param = softplus(pyro.param(\"out_bias_scale\", out_bias_scale))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check altered parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.param('fc1_weight_loc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.param('fc1_weight_scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Bayesian neural network\n",
    "In contrast to classical neural network with fixed parameters, Bayesian neural network with its random parameters need multiple runs / forwardpasses for a single input to predict an output. For an identical input file, each run will generate a different output. This is due to the randomness of the *weights* and *biases* of the Bayesian neural network. This is displayed in the following drawing from [Blundell et al](https://arxiv.org/pdf/1505.05424.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../pictures/Blundell_1.png' width=600 align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width: 600px; text-align: center;'>Blundell et al.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left you can see a classical neural net with fixed *weights* and *biases* and on the right side you can see a Bayesian neural net with its random parameters.\n",
    "\n",
    "Therefore you need multiple runs to calculate the outcome.\n",
    "\n",
    "In the implementation below we use the *guide()* for prediction because it has the trained parameters.\n",
    "1. The first statement within *prediction()* generates *num_of_samples* Bayesian networks\n",
    "2. The second statement generates *num_of_samples* predictions for each single input. Thus if we have a test batch size of 64 we get in total 640 predictions, 10 for each input.\n",
    "3. The forth statement calculates the mean / average over the *num_of_samples* output for each input and each category. Thus yielding in 64 tensors with a mean value for each category.\n",
    "4. The result is the index of the category with the maximal mean value. There is one value for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of multiple runs for a single input\n",
    "num_of_samples = 100\n",
    "\n",
    "def predict(x):\n",
    "    # generate num_of_samples models \n",
    "    sampled_models = [guide(None, None) for _ in range(num_of_samples)]\n",
    "    # forwardpass of the date -> generates num_of_samples predictions per input\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    # stack the tensors for further processing\n",
    "    inter = torch.stack(yhats)\n",
    "    # Build the mean value for each category out of the sampled models\n",
    "    mean = torch.mean(inter, 0).cpu()\n",
    "    # Build the variance for each category out of the sampled models\n",
    "    variance = torch.var(inter, 0).cpu()\n",
    "    # get the category with the maximal mean value\n",
    "    # this is the predicted value of the Bayesian natwork\n",
    "    res = np.argmax(mean.numpy(), axis=1)\n",
    "    return res\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for j, data in enumerate(testloader):\n",
    "    images, labels = data\n",
    "    # move data to cuda\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        predicted = predict(images.view(-1,28*28))\n",
    "    total += labels.size(0)\n",
    "    correct += np.equal(predicted, labels).sum()\n",
    "    \n",
    "    if j % 10 == 0:\n",
    "        print('{} out of {} test batches'.format(j, len(testloader)))\n",
    "        \n",
    "print(\"Accuracy without confidence estimation: {:.1f} %\".format(correct / total * 100))\n",
    "\n",
    "print(\"\\nTime for testimg whole testing set: {:.0f} minutes and {:.3f} seconds\"\n",
    "      .format((time.time() - start)/60, (time.time() - start) % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance of each category\n",
    "The following function `calculate_variance()` calculates the predicted variance of each category. \n",
    "1. For one input make `num_of_samples` forward passes through `num_of_samples` different models.\n",
    "2. Calculate the variance for each category over the `num_of_samples` results.\n",
    "3. Calculate a mean variance of the category variances.\n",
    "4. \"Normalize\" each category variance with the mean variance.\n",
    "5. Extract the minimal normalized category variance and return it.\n",
    "6. Extract the category with the minimal normalized category variance and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of multiple runs for a single input\n",
    "num_of_samples = 100\n",
    "\n",
    "def calculate_variance(x):\n",
    "    # generate num_of_samples models \n",
    "    sampled_models = [guide(None, None) for _ in range(num_of_samples)]\n",
    "    \n",
    "    # calculate the log softmax probability for the input images\n",
    "    yhats = [F.log_softmax(model(x.view(-1,28*28)).data, 1).cpu().numpy() for model in sampled_models]\n",
    "    \n",
    "    # 1. Convert yhat into an array -> np.asarray()\n",
    "    # 2. Join the sequence of arrays along axis 1 -> np.stack\n",
    "    # 3. Calculet the variance along axis 1 for each category out of the num_of_samples results-> np.var\n",
    "    var_yhat = np.var(np.stack(np.asarray(yhats), 1), 1)\n",
    "    \n",
    "    # Average the variences\n",
    "    mean_var = np.mean(var_yhat, 1)\n",
    "    \n",
    "    interm = []\n",
    "    for i in range(len(mean_var)):\n",
    "        interm.append(var_yhat[i] / mean_var[i])\n",
    "    \n",
    "    # Array of the normalized categy variances \n",
    "    normalized_var = np.array(interm)\n",
    "    \n",
    "    # Get the minimal categorical value\n",
    "    min_var = np.min(normalized_var, 1)\n",
    "    \n",
    "    # Get the index (the category) with the minimal value\n",
    "    min_var_item = np.argmin(normalized_var, 1)\n",
    "    \n",
    "    # Return the minimal normelized categary variance and the corresponding category\n",
    "    return min_var, min_var_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_var_batch(images, labels, plot=True):\n",
    "    images = images.to(device)\n",
    "    min_var, min_var_item = calculate_variance(images)\n",
    "    images = images.cpu()\n",
    "    predicted_for_images = 0\n",
    "    correct_predictions = 0\n",
    "    normalized_var_threshold = 0.2\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "    \n",
    "        if(plot):\n",
    "            print(\"Real: \",labels[i].item())\n",
    "    \n",
    "        highted_something = False\n",
    "        \n",
    "        if(plot):\n",
    "            plt.show()\n",
    "    \n",
    "        if min_var[i] < normalized_var_threshold:\n",
    "            highted_something = True\n",
    "            predicted = min_var_item[i]\n",
    "            \n",
    "    \n",
    "        if(highted_something):\n",
    "            predicted_for_images+=1\n",
    "            if(labels[i].item()==predicted):\n",
    "                if(plot):\n",
    "                    print(\"Correct\")\n",
    "                correct_predictions +=1.0\n",
    "            else:\n",
    "                if(plot):\n",
    "                    print(\"Incorrect\")\n",
    "        else:\n",
    "            if(plot):\n",
    "                print(\"Undecided.\")\n",
    "        if(plot):\n",
    "                print(\"Normalized Variance = {:.5f} for threshold = {:.2f}\"\n",
    "                      .format(min_var[i], normalized_var_threshold))\n",
    "        if(plot):\n",
    "            helper.small_imshow(images[i].squeeze())\n",
    "        \n",
    "    \n",
    "    if(plot):\n",
    "        print(\"\\n\\nSummary\")\n",
    "        print(\"Number of images: \",len(labels))\n",
    "        print(\"Predicted for: \",predicted_for_images)\n",
    "        if predicted_for_images > 0:\n",
    "            print(\"Accuracy when predicted: \",correct_predictions/predicted_for_images)\n",
    "        else:\n",
    "            print(\"No prediction therefore no accuracy value available.\")\n",
    "        \n",
    "    return len(labels), correct_predictions, predicted_for_images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test procedure with the `MNIST` test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "test_var_batch(images[:60], labels[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-distribution test\n",
    "Load the `FashionMNIST` test set to perform an out-of-distribution test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FashionMNIST test dataset\n",
    "fashiontestset = torchvision.datasets.FashionMNIST('../data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashiontestloader = torch.utils.data.DataLoader(fashiontestset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test procedure with the `FashionMNIST` test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(fashiontestloader))\n",
    "test_var_batch(images[0:60], labels[0:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of the test procedure for `MNIST` and `FashionMNIST`\n",
    "The results above illustrate the intention behind the confidence calculation. For the `MNIST` data the confidence calculation leads the an accuracy of 100 % and the algorithm was confident at around 80 % of the input images. It detected 20 % of the input images not as digits and therefore the **false negative** (type 2 error) rate for confidence calculation is 20 %.\n",
    "\n",
    "On the other hand, it detected about 80 % as non-digits for the `FashionMNIST` data set. Therefore the **false positive** (type 1 error) rate for confidence calculation is 20 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction after confidence estimation\n",
    "print('Prediction after confidence estimation')\n",
    "correct = 0\n",
    "total = 0\n",
    "total_predicted_for = 0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for j, data in enumerate(testloader):\n",
    "    images, labels = data\n",
    "    \n",
    "    total_minibatch, correct_minibatch, predictions_minibatch = test_var_batch(images, labels, plot=False)\n",
    "    total += total_minibatch\n",
    "    correct += correct_minibatch\n",
    "    total_predicted_for += predictions_minibatch\n",
    "    \n",
    "    if j % 10 == 0:\n",
    "        print('{} out of {} test batches'.format(j, len(testloader)))\n",
    "\n",
    "print(\"Total images: \", total)\n",
    "print(\"Skipped: \", total-total_predicted_for)\n",
    "print(\"Accuracy when made predictions: {:3.1f}\".format (100 * correct / total_predicted_for))\n",
    "\n",
    "print(\"\\nTime for testing confidence estimation of whole testing set : {:.0f} minutes and {:.3f} seconds\"\n",
    "      .format((time.time() - start)/60, (time.time() - start) % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and future improvements \n",
    "The accuracy after confidence estimation is 98.8 % and therefore an improvement of 7.4 % compared to the prediction without confidence estimation. But the rate of the *false negative* or *type 2* errors for confidence estimation is pretty high with 27.52 %. Thus it is important to lower the type 2 error rate of the confidence estimation by keeping the accuracy as high as possible. \n",
    "\n",
    "#### Possible technical improvements\n",
    "1. average the variance over all but the smallest values.\n",
    "2. Calculate the distance between the smallest and the second smallest value.\n",
    "3. Take multiple runs (eg. 5) and average over them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[[1]](https://www.youtube.com/watch?v=DYRK0-_K2UU&feature=youtu.be) Tamara Broderick - Tutorial \"Variational Bayes and beyond: Bayesian inference for big data\" - ICML 2018.\n",
    "<br>\n",
    "[[2]](https://arxiv.org/pdf/1505.05424.pdf) C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra, “Weight uncertainty in neural networks,” in *International Conference on Machine Learning*, 2015, pp. 1613–1622.\n",
    "<br>\n",
    "[[3]](https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd) Paras Chopra, \"Making Your Neural Network Say “I Don’t Know” — Bayesian NNs using Pyro and PyTorch\" in *towards data science*, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
